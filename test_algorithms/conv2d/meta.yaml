algorithm:
  name: "Conv2D"
  description: "2D Convolutional Neural Network Layer with ReLU activation and fixed-point arithmetic"
  version: "1.0"
  author: "ARDA Pipeline"
  license: "MIT"

interface:
  input:
    type: "feature_map_3d"
    dimensions: [8, 8, 3]  # Height, Width, Channels
    data_type: "float32"
    description: "8x8 RGB-like input feature map"

  output:
    type: "feature_map_3d"
    dimensions: [6, 6, 16]  # Height, Width, Channels (after 3x3 conv with padding)
    data_type: "int8"
    description: "6x6 output feature map with 16 channels"

  step_function: "conv2d_step_function"

performance:
  target_frequency_mhz: 200.0
  throughput_samples_per_second: 20000000  # 20M inferences/sec
  latency_cycles_max: 1024  # Allow pipelined implementation
  power_budget_mw: 300

resource_budget:
  lut: 10000
  ff: 15000
  dsp: 32
  bram: 8

fp_config:
  input_bits: 8
  input_frac_bits: 6
  weight_bits: 8
  weight_frac_bits: 6
  bias_bits: 16
  bias_frac_bits: 12
  output_bits: 8
  output_frac_bits: 6

implementation:
  architecture: "parallel_conv"
  parallelism: 16  # 16 output channels computed in parallel
  kernel_size: 3
  stride: 1
  padding: 1
  activation: "relu"
  memory_type: "bram"  # For weight storage
  optimization_goals:
    - "throughput_maximization"
    - "resource_efficiency"
    - "fixed_point_precision"

verification:
  test_vectors_count: 50
  accuracy_threshold_db: 30  # Neural network precision requirements
  corner_cases:
    - "zero_input"
    - "saturated_input"
    - "edge_patterns"
    - "noise_input"
  golden_reference: "numpy_convolution"

axi_interface:
  data_width: 64  # 8 channels * 8 bits
  addr_width: 10  # log2(8*8*3) ~ 8, but allow for larger
  max_burst_length: 192  # 8*8*3 input elements
